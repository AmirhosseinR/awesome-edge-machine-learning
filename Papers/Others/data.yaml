# OTHERS
#
# `date` format is as following yyy/mm/dd. For example 6 May 2019 would be 2019/05/06.
# In case of arxiv, use the date of the first version of paper.
#
# [Template]
#
# -
#   name:
#   url:
#   date:
#   conference:
#   code:
#   authors:
#   abstract:
---

-
  name: >
    Machine Learning at Facebook:Understanding Inference at the Edge
  url: https://research.fb.com/wp-content/uploads/2018/12/Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge.pdf
  date: 2018/12/01
  conference:
  code:
  authors: Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan,Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, Tommer Leyvand, Hao Lu, Yang Lu, Lin Qiao,Brandon Reagen, Joe Spisak, Fei Sun, Andrew Tulloch, Peter Vajda, Xiaodong Wang,Yanghan Wang, Bram Wasti, Yiming Wu, Ran Xian, Sungjoo Yoo, Peizhao Zhang
  abstract: >
    At Facebook, machine learning provides a wide range ofcapabilities that drive many aspects of user experienceincluding ranking posts, content understanding, objectdetection and tracking for augmented and virtual real-ity, speech and text translations.  While machine learn-ing  models  are  currently  trained  on  customized  data-center infrastructure, Facebook is working to bring ma-chine learning inference to the edge.  By doing so, userexperience is improved with reduced latency (inferencetime) and becomes less dependent on network connec-tivity.  Furthermore, this also enables many more appli-cations  of  deep  learning  with  important  features  onlymade available at the edge.  This paper takes a data-driven  approach  to  present  the  opportunities  and  de-sign  challenges  faced  by  Facebook  in  order  to  enablemachine learning inference locally on smartphones andother edge platforms.

-
  name: >
    On-Device Neural Net Inference with Mobile GPUs
  url: https://arxiv.org/abs/1907.01989
  date: 2019/07/03
  conference:
  code:
  authors: Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pisarchyk, Mogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei Kulik, Matthias Grundmann
  abstract: >
    On-device inference of machine learning models for mobile phones is desirable due to its lower latency and increased privacy. Running such a compute-intensive task solely on the mobile CPU, however, can be difficult due to limited computing power, thermal constraints, and energy consumption. App developers and researchers have begun exploiting hardware accelerators to overcome these challenges. Recently, device manufacturers are adding neural processing units into high-end phones for on-device inference, but these account for only a small fraction of hand-held devices. In this paper, we present how we leverage the mobile GPU, a ubiquitous hardware accelerator on virtually every phone, to run inference of deep neural networks in real-time for both Android and iOS devices. By describing our architecture, we also discuss how to design networks that are mobile GPU-friendly. Our state-of-the-art mobile GPU inference engine is integrated into the open-source project TensorFlow Lite and publicly available at [https://tensorflow.org/lite](https://tensorflow.org/lite).

...
